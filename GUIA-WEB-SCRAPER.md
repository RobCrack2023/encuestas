# Gu√≠a del Web Scraper de Noticias

Sistema autom√°tico de recolecci√≥n de noticias pol√≠ticas y electorales desde m√∫ltiples fuentes de medios chilenos.

## üéØ Caracter√≠sticas

- ‚úÖ Scraping autom√°tico de noticias desde m√∫ltiples fuentes
- ‚úÖ Soporte para RSS y scraping HTML
- ‚úÖ Filtrado por palabras clave (elecciones, pol√≠tica, etc.)
- ‚úÖ Almacenamiento en base de datos
- ‚úÖ Detecci√≥n de noticias duplicadas
- ‚úÖ Interfaz visual atractiva en el frontend
- ‚úÖ Actualizaci√≥n manual o autom√°tica

## üì∞ Fuentes Configuradas

El sistema viene preconfigurado con:

1. **BioB√≠o Chile** (RSS)
   - Feed de pol√≠tica

2. **T13** (RSS)
   - Noticias pol√≠ticas

3. **Emol** (HTML Scraping)
   - Secci√≥n nacional

4. **La Tercera** (HTML Scraping)
   - Secci√≥n pol√≠tica

## üöÄ Uso B√°sico

### Desde la Interfaz Web

1. Ve a la secci√≥n **Noticias** en el men√∫
2. Haz clic en **"Actualizar Noticias"**
3. El sistema descargar√° autom√°ticamente las √∫ltimas noticias
4. Filtra por fuente si deseas

### Desde la API

```bash
# Actualizar noticias manualmente
curl -X POST http://localhost:5000/api/noticias/actualizar

# Obtener noticias almacenadas
curl http://localhost:5000/api/noticias

# Obtener noticias de una fuente espec√≠fica
curl http://localhost:5000/api/noticias?source=biobio

# Limitar n√∫mero de resultados
curl http://localhost:5000/api/noticias?limit=10

# Obtener lista de fuentes
curl http://localhost:5000/api/noticias/fuentes
```

## ‚öôÔ∏è Agregar Nuevas Fuentes

### Fuente RSS

Edita `backend/scraper/news_scraper.py` y agrega en `self.sources`:

```python
'nueva_fuente': {
    'name': 'Nombre del Medio',
    'rss': 'https://ejemplo.cl/feed/rss',
    'type': 'rss',
    'logo': '/images/logo.png'
}
```

### Fuente HTML

```python
'nueva_fuente': {
    'name': 'Nombre del Medio',
    'url': 'https://ejemplo.cl/politica',
    'type': 'html',
    'logo': '/images/logo.png'
}
```

**Nota**: El scraper HTML usa estrategias gen√©ricas. Para sitios espec√≠ficos, puede que necesites personalizar el m√©todo `_scrape_html()`.

## üîÑ Actualizaci√≥n Autom√°tica

### Opci√≥n 1: Cron Job (Recomendado para VPS)

Crea un cron job para ejecutar cada hora:

```bash
crontab -e
```

Agrega:

```bash
0 * * * * curl -X POST http://localhost:5000/api/noticias/actualizar
```

### Opci√≥n 2: APScheduler (Integrado en Flask)

Agrega al final de `backend/app.py` antes de `if __name__ == '__main__'`:

```python
from apscheduler.schedulers.background import BackgroundScheduler

def actualizar_noticias_automatico():
    with app.app_context():
        try:
            from scraper.news_scraper import get_political_news
            noticias_scraped = get_political_news(limit=50)

            for noticia_data in noticias_scraped:
                existente = Noticia.query.filter_by(url=noticia_data['url']).first()
                if not existente:
                    nueva_noticia = Noticia(**noticia_data)
                    db.session.add(nueva_noticia)

            db.session.commit()
            print("Noticias actualizadas autom√°ticamente")
        except Exception as e:
            print(f"Error actualizando noticias: {str(e)}")

# Programar actualizaci√≥n cada hora
scheduler = BackgroundScheduler()
scheduler.add_job(func=actualizar_noticias_automatico, trigger="interval", hours=1)
scheduler.start()
```

## üé® Personalizaci√≥n

### Cambiar Palabras Clave de Filtrado

Edita `backend/scraper/news_scraper.py`, funci√≥n `get_political_news()`:

```python
keywords = [
    'elecciones',
    'presidencial',
    'candidato',
    'tu_palabra_clave',
    # Agrega m√°s...
]
```

### Modificar L√≠mite de Noticias

En la interfaz web o v√≠a API:

```javascript
// Frontend: src/pages/Noticias.jsx
const response = await axios.get('/api/noticias', {
  params: { limit: 50 }  // Cambiar aqu√≠
})
```

### Agregar Fuentes Internacionales

```python
# En news_scraper.py
'bbc_mundo': {
    'name': 'BBC Mundo',
    'rss': 'https://www.bbc.com/mundo/topics/...',
    'type': 'rss',
    'logo': '/images/bbc-logo.png'
}
```

## üõ†Ô∏è Instalaci√≥n en VPS

### 1. Instalar Dependencias

```bash
cd /var/www/encuestas/backend
source venv/bin/activate
pip install beautifulsoup4 requests feedparser APScheduler
```

O simplemente:

```bash
pip install -r requirements.txt
```

### 2. Actualizar Base de Datos

```bash
# Crear las tablas de noticias
curl -X POST http://localhost:5000/api/init-db
```

### 3. Primera Carga de Noticias

```bash
curl -X POST http://localhost:5000/api/noticias/actualizar
```

### 4. Verificar

```bash
curl http://localhost:5000/api/noticias
```

### 5. Build del Frontend

```bash
cd /var/www/encuestas/frontend
npm run build
sudo systemctl reload nginx
```

## üìä Estructura de Datos

### Modelo de Noticia

```python
class Noticia:
    id: int
    title: str                  # T√≠tulo de la noticia
    url: str                    # URL √∫nica de la noticia
    summary: str                # Resumen o extracto
    published_at: datetime      # Fecha de publicaci√≥n
    source: str                 # Nombre del medio
    source_id: str              # ID de la fuente
    source_logo: str            # URL del logo
    image_url: str              # URL de la imagen
    created_at: datetime        # Cu√°ndo se agreg√≥ a la BD
    is_active: bool             # Si est√° activa o no
```

## üö® Troubleshooting

### Error: "Module 'scraper' not found"

```bash
# Aseg√∫rate de que existe __init__.py
touch backend/scraper/__init__.py
```

### Error: "No se pueden obtener noticias"

1. **Verifica conectividad**:
   ```bash
   curl https://www.biobiochile.cl
   ```

2. **Revisa logs del backend**:
   ```bash
   sudo journalctl -u encuestas -f
   ```

3. **Prueba el scraper directamente**:
   ```python
   cd backend
   source venv/bin/activate
   python
   >>> from scraper.news_scraper import get_political_news
   >>> news = get_political_news(limit=5)
   >>> print(news)
   ```

### Noticias Duplicadas

El sistema detecta duplicados por URL. Si ves duplicados:

```sql
-- Limpiar duplicados en PostgreSQL
DELETE FROM noticias
WHERE id NOT IN (
    SELECT MIN(id)
    FROM noticias
    GROUP BY url
);
```

### Scraper muy lento

Reduce el l√≠mite de noticias:

```python
# En news_scraper.py, m√©todo scrape_all()
news = self._scrape_html(source_id, source_config, limit)
# Cambia limit a un valor menor, ej: 10
```

## üìã Endpoints API

| M√©todo | Endpoint | Descripci√≥n |
|--------|----------|-------------|
| GET | `/api/noticias` | Obtiene noticias almacenadas |
| GET | `/api/noticias?source=biobio` | Filtra por fuente |
| GET | `/api/noticias?limit=20` | Limita resultados |
| POST | `/api/noticias/actualizar` | Ejecuta el scraper |
| GET | `/api/noticias/fuentes` | Lista fuentes configuradas |

## üîí Consideraciones de Seguridad

1. **Rate Limiting**: Los sitios pueden bloquear IPs con demasiadas solicitudes
2. **Robots.txt**: Respeta las pol√≠ticas de los sitios
3. **User Agent**: El scraper usa un User-Agent gen√©rico
4. **Legal**: Aseg√∫rate de cumplir con t√©rminos de servicio

## üí° Tips

1. **No hagas scraping muy frecuente**: Cada hora es suficiente
2. **Revisa las noticias peri√≥dicamente**: Elimina noticias antiguas
3. **Logs**: Monitorea errores de scraping
4. **Im√°genes**: Algunas pueden no cargar (protecci√≥n anti-hotlink)
5. **Backup**: Exporta noticias importantes antes de limpiar la BD

## üéØ Futuras Mejoras

- [ ] Categorizaci√≥n autom√°tica con IA
- [ ] Res√∫menes generados con LLM
- [ ] An√°lisis de sentimiento
- [ ] Detecci√≥n de fake news
- [ ] Notificaciones push de noticias importantes
- [ ] Widget de noticias en la p√°gina principal
- [ ] Exportar noticias a PDF/Excel

---

¬øNecesitas ayuda con el web scraper? Consulta los logs o crea un issue en GitHub.
